import pandas as pd
import requests
import re
import time
from bs4 import BeautifulSoup

notes = []

def get_page(url):
    """Get webpage content"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error: {e}")
        return None

def extract_temp_value(temp_str):
    match = re.search(r'(-?\d+(?:\.\d+)?)', temp_str)
    return float(match.group(1)) if match else None


def find_specs_table(soup):
    """Find and extract only the specifications table"""

    # Look for tables containing specification data
    tables = soup.find_all('table')

    for table in tables:
        table_text = table.get_text().lower()

        # Check if table contains specification keywords
        spec_keywords = ['capacitance', 'voltage', 'tolerance', 'temperature',
                        'specification', 'parameter', 'attribute', 'property']

        if any(keyword in table_text for keyword in spec_keywords):
            # Extract table data
            rows = table.find_all('tr')
            spec_data = []

            for row in rows:
                cells = row.find_all(['td', 'th'])
                row_data = []

                for cell in cells:
                    cell_text = cell.get_text().strip()
                    if cell_text:
                        row_data.append(cell_text)

                if len(row_data) == 2:  # Only keep attribute-value pairs
                    spec_data.append(row_data)

            if spec_data:
                return spec_data

    return None

def print_specs_table(specs):
    """Print only the specifications table"""
    if not specs:
        print("No specifications table found")
        return

    print("SPECIFICATIONS")
    print("-" * 50)

    for attribute, value in specs:
        print(f"{attribute:<30} : {value}")
def normalize_temperature_comprehensive(temp_str):
    """Comprehensive normalization for temperature strings"""
    if not temp_str:
        return ""

    # Convert to string and strip whitespace
    temp = str(temp_str).strip()

    # Handle different degree symbol variations
    temp = temp.replace('°', '°')  # Normalize degree symbol if needed

    # Remove spaces around degree symbol and temperature unit
    temp = re.sub(r'\s*°\s*C', '°C', temp, flags=re.IGNORECASE)
    temp = re.sub(r'\s*°\s*F', '°F', temp, flags=re.IGNORECASE)

    # Remove extra spaces
    temp = re.sub(r'\s+', ' ', temp).strip()

    return temp
def enhanced_temperature_matcher(temp1, temp2):
    """Enhanced matcher with comprehensive normalization"""
    norm1 = normalize_temperature_comprehensive(temp1)
    norm2 = normalize_temperature_comprehensive(temp2)

    return norm1.lower() == norm2.lower()
def validate_capacitor_spec(specs, Valid_data):
    results = {}
    global notes

    # Process each spec from the scraped data
    for key, value in specs:
        matched = False

        # Check against each valid data entry
        for fet in Valid_data:
            mapping_feature = fet['Mapping_Feature'].lower().strip()
            valid_value = str(fet['Value'])
            key_lower = key.lower().strip()
            value_lower = value.lower().strip()

            # Check if the mapping feature matches the key
            if key_lower == valid_value:
                # results[key] = f"{value}, Valid"
                notes.append(f"{key} : {value} is Valid (exact match) con1")
                matched = True
                break

            elif ('°C' in valid_value or '°F' in valid_value or
                  '°C' in value_lower or '°F' in value_lower):
                if enhanced_temperature_matcher(valid_value, valid_value):
                    # results[key] = f"{value}, Valid"
                    notes.append(f"{key} : {value} is Valid (temperature match)con2")
                    matched = True
                    break
            elif key_lower == mapping_feature:
                # Check for exact match
                if value_lower == valid_value:
                    # results[key] = f"{value}, Valid"
                    notes.append(f"{key} : {value} is Valid (Value is Yes) con3")
                    matched = True
                    break
                # Check if value contains the valid value
                elif valid_value in value_lower:
                    # results[key] = f"{value}, Valid"
                    notes.append(f"{key} : {value} contains valid value con2 {fet['Value']} con4")
                    matched = True
                    break

        # If no match found, mark as not valid
        if not matched:
            results[key] = f"{value}, Not Valid"
            notes.append(f"{key} : {value} is Not Valid - no matching {fet['Mapping_Feature']} : {fet['Value']} criteria found")
        elif  matched:
            results[key] = f"{value}, Valid"
            notes.append(f"{key} : {value} is Valid (exact match)")
    return results

def extract_mapping_features(csv_path, target_part_number):
    """
    Reads a large CSV file in chunks and extracts 'Mapping Feature (FindChip)' and 'Value'
    for a specific 'Part Number'.
    """
    try:
        df_large = pd.read_csv(csv_path, chunksize=100000)
        data = []

        for chunk in df_large:
            # Find rows with the target part number
            matching_rows = chunk[chunk['Part Number'] == target_part_number]

            if not matching_rows.empty:
                for _, row in matching_rows.iterrows():
                    data.append({
                        "Mapping_Feature": row['Mapping Feature (FindChip)'],
                        "Value": row['Value']
                    })
                break  # Found the part, no need to continue

        return data
    except Exception as e:
        print(f"Error reading CSV for part {target_part_number}: {e}")
        return []

def extract_mapping_features_c_p(csv_path):
    """Extract company and part number data from CSV"""
    try:
        df_large = pd.read_csv(csv_path, chunksize=100000)
        data = []

        for chunk in df_large:
            for _, row in chunk.iterrows():
                data.append({
                    "Company Name": row['Company Name'],
                    "Part Number": row['Part Number']
                })

        return data
    except Exception as e:
        print(f"Error reading CSV: {e}")
        return []

def save_specs_table(specs, filename):
    """Save only the specifications table to file"""
    if not specs:
        return

    with open(filename, 'w', encoding='utf-8') as f:
        f.write("SPECIFICATIONS\n")
        f.write("-" * 50 + "\n")
        for attribute, value in specs:
            f.write(f"{attribute:<30} : {value}\n")

def main():
    # Initialize the CSV file with headers
    with open("Mapping_Features_Solution.csv", 'w', encoding='utf-8') as f:
        f.write("Company,Part Number,Features(FC),Value from findchip,Features(V),Value(V),Validation Status,Notes\n")

    # Extract company and part number data
    data1 = extract_mapping_features_c_p("/content/Mapping_Features_1.csv")
    print(f"Total records to process: {len(data1)}")

    processed_count = 0

    for item in data1:
        # Uncomment the next two lines if you want to limit processing for testing
        # if processed_count >= 5:  # Process only first 5 for testing
        #     break

      part_number = item["Part Number"].strip()
      company = item["Company Name"]
      # part_number = "C0402C0G1C050C020BC"
      # company = "TDK Corporation"
      print(f"\nProcessing {processed_count + 1}/{len(data1)}: Part Number {part_number} from {company}")

      # Construct URL
      url = f"https://www.findchips.com/detail/{part_number.lower()}/{company}"

      # Get webpage
      html = get_page(url)
      if not html:
          print(f"Failed to get page for {part_number}")

      soup = BeautifulSoup(html, 'html.parser')

      # Find specifications table
      specs = find_specs_table(soup)
      if not specs:
          print("No specifications table found in findchip.")
          continue

      # Get validation data for this part
      # data = extract_mapping_features("D:\\python-env\\Mapping_Features_1.csv", part_number)
      data = extract_mapping_features("/content/Mapping_Features_1.csv", part_number)

      if not data:
          print(f"No validation data found for {part_number}")
          continue

      # print("Validation data found:")
      # for i in data:
      #     print(f"  {i['Mapping_Feature']} : {i['Value']}")

      # Validate specifications
      results = validate_capacitor_spec(specs, data)

      print("\nValidation Results:")
      for key, value in results.items():
          print(f"  {key}: {value}")

      # Print specifications table
      print("\nSPECIFICATIONS FROM FINDCHIP:")
      print_specs_table(specs)

      # Save data to CSV file
      with open("Mapping_Features_Solution.csv", 'a', encoding='utf-8') as f:
          # Create a set to track which validation data we've used
          used_validation_items = set()

          # First, write rows for specs that have matches
          for spec_key, spec_value in specs:
              validation_status = results.get(spec_key, "No validation data")

              # Find corresponding mapping feature from validation data
              mapping_feature = ""
              mapping_value = ""
              for i, valid_item in enumerate(data):
                  if valid_item['Mapping_Feature'].lower().strip() == spec_key.lower().strip():
                      mapping_feature = valid_item['Mapping_Feature']
                      mapping_value = valid_item['Value']
                      used_validation_items.add(i)
                      break

              # Collect relevant notes for this specification
              relevant_notes = [note for note in notes if spec_key.lower() in note.lower()]
              notes_text = "; ".join(relevant_notes) if relevant_notes else ""

              # Write to CSV
              f.write(f'"{company}","{part_number}","{spec_key}","{spec_value}","{mapping_feature}","{mapping_value}","{validation_status}","{notes_text}"\n')
              print(f"Saved: {company}, {part_number}, {spec_key}, {spec_value}, {mapping_feature}, {mapping_value}, {validation_status}")

          # Then, write rows for validation data that didn't match any specs
          for i, valid_item in enumerate(data):
              if i not in used_validation_items:
                  mapping_feature = valid_item['Mapping_Feature']
                  mapping_value = valid_item['Value']
                  validation_status = "No matching spec found"
                  notes_text = f"Validation data '{mapping_feature}: {mapping_value}' had no corresponding specification in scraped data"

                  # Write to CSV with empty spec fields
                  # f.write(f'"{company}","{part_number}","","","{mapping_feature}","{mapping_value}","{validation_status}","{notes_text}"\n')
                  f.write(f'"{company}","{part_number}","{spec_key}","{spec_value}","{mapping_feature}","{mapping_value}","{validation_status}","{notes_text}"\n')
                  print(f"Saved validation data: {company}, {part_number},  {mapping_feature}, {mapping_value}, {validation_status}")

      print(f"Data saved for {part_number}")
      processed_count += 1

      # Clear notes for next iteration
      notes.clear()

      # Add a small delay to be respectful to the website
      # time.sleep(1)

      print(f"\nProcessing complete! Processed {processed_count} parts.")
      print("Results saved to: Mapping_Features_Solution.csv")

if __name__ == "__main__":
    main()
